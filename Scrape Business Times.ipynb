{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be0d327c-7100-455a-9e06-3714a714a1ee",
   "metadata": {},
   "source": [
    "# scrape list of BT articles from BT housing page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f5e3f7b-61c0-496e-804c-42f6eacfef1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Batch 1 summary ---\n",
      "New links discovered:      139\n",
      "Meta-backed links checked: 30\n",
      "Kept (in window):          15\n",
      "Dropped before window:     0\n",
      "Dropped after window:      15\n",
      "No published_time meta:    100\n",
      "Errors fetching articles:  0\n",
      "Oldest date this batch:    2025-07-14\n",
      "Global oldest seen:        2025-07-14\n",
      "\n",
      "--- Batch 2 summary ---\n",
      "New links discovered:      10\n",
      "Meta-backed links checked: 10\n",
      "Kept (in window):          10\n",
      "Dropped before window:     0\n",
      "Dropped after window:      0\n",
      "No published_time meta:    0\n",
      "Errors fetching articles:  0\n",
      "Oldest date this batch:    2025-06-13\n",
      "Global oldest seen:        2025-06-13\n",
      "\n",
      "--- Batch 3 summary ---\n",
      "New links discovered:      10\n",
      "Meta-backed links checked: 10\n",
      "Kept (in window):          10\n",
      "Dropped before window:     0\n",
      "Dropped after window:      0\n",
      "No published_time meta:    0\n",
      "Errors fetching articles:  0\n",
      "Oldest date this batch:    2025-04-28\n",
      "Global oldest seen:        2025-04-28\n",
      "\n",
      "--- Batch 4 summary ---\n",
      "New links discovered:      10\n",
      "Meta-backed links checked: 10\n",
      "Kept (in window):          10\n",
      "Dropped before window:     0\n",
      "Dropped after window:      0\n",
      "No published_time meta:    0\n",
      "Errors fetching articles:  0\n",
      "Oldest date this batch:    2025-02-13\n",
      "Global oldest seen:        2025-02-13\n",
      "\n",
      "--- Batch 5 summary ---\n",
      "New links discovered:      10\n",
      "Meta-backed links checked: 10\n",
      "Kept (in window):          10\n",
      "Dropped before window:     0\n",
      "Dropped after window:      0\n",
      "No published_time meta:    0\n",
      "Errors fetching articles:  0\n",
      "Oldest date this batch:    2025-01-14\n",
      "Global oldest seen:        2025-01-14\n",
      "\n",
      "--- Batch 6 summary ---\n",
      "New links discovered:      10\n",
      "Meta-backed links checked: 10\n",
      "Kept (in window):          3\n",
      "Dropped before window:     7\n",
      "Dropped after window:      0\n",
      "No published_time meta:    0\n",
      "Errors fetching articles:  0\n",
      "Oldest date this batch:    2024-11-25\n",
      "Global oldest seen:        2024-11-25\n",
      "\n",
      "--- Batch 7 summary ---\n",
      "New links discovered:      10\n",
      "Meta-backed links checked: 10\n",
      "Kept (in window):          0\n",
      "Dropped before window:     10\n",
      "Dropped after window:      0\n",
      "No published_time meta:    0\n",
      "Errors fetching articles:  0\n",
      "Oldest date this batch:    2024-10-14\n",
      "Global oldest seen:        2024-10-14\n",
      "\n",
      "--- Batch 8 summary ---\n",
      "New links discovered:      10\n",
      "Meta-backed links checked: 10\n",
      "Kept (in window):          0\n",
      "Dropped before window:     10\n",
      "Dropped after window:      0\n",
      "No published_time meta:    0\n",
      "Errors fetching articles:  0\n",
      "Oldest date this batch:    2024-09-04\n",
      "Global oldest seen:        2024-09-04\n",
      "\n",
      "Reached older-than-start dates and no new in-window items for 2 batches. Stopping.\n",
      "\n",
      "=== Overall ===\n",
      "Total kept (in window): 58\n",
      "Saved to: bt_residential_2025-01-01_to_2025-09-30.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "from datetime import datetime, date\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# --- Selenium imports ---\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ================= Config =================\n",
    "START_DATE = date(2025, 1, 1)\n",
    "END_DATE   = date(2025, 9, 30)\n",
    "\n",
    "LISTING_URL = \"https://www.businesstimes.com.sg/property/residential\"\n",
    "OUT_CSV     = \"Output/bt_residential_2025-01-01_to_2025-09-30.csv\"\n",
    "\n",
    "# How many scroll-batches at most (safety cap)\n",
    "MAX_SCROLL_BATCHES = 60\n",
    "\n",
    "# Pause timings\n",
    "SCROLL_PAUSE_SEC = 1.2          # pause after each scroll\n",
    "NEW_CARDS_SETTLE_SEC = 1.5      # give time for new cards to render\n",
    "REQUEST_TIMEOUT = 20\n",
    "PAUSE_BETWEEN_ARTICLE_REQUESTS = 0.3\n",
    "\n",
    "# Accept these meta slots as \"published_time\"\n",
    "PUBLISHED_META_SLOTS = [\n",
    "    (\"property\", \"article:published_time\"),\n",
    "    (\"name\",     \"article:published_time\"),\n",
    "    (\"name\",     \"parsely-pub-date\"),\n",
    "    (\"itemprop\", \"datePublished\"),\n",
    "]\n",
    "\n",
    "SGT_TZ = \"Asia/Singapore\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# ================= Helpers =================\n",
    "def to_abs(base, href):\n",
    "    if not href:\n",
    "        return None\n",
    "    return href if href.startswith((\"http://\", \"https://\")) else urljoin(base, href)\n",
    "\n",
    "def same_domain(u):\n",
    "    p = urlparse(u)\n",
    "    return p.netloc in {\"www.businesstimes.com.sg\", \"businesstimes.com.sg\"}\n",
    "\n",
    "def find_published_meta(soup):\n",
    "    for attr, val in PUBLISHED_META_SLOTS:\n",
    "        tag = soup.find(\"meta\", attrs={attr: val})\n",
    "        if tag and tag.get(\"content\"):\n",
    "            return tag[\"content\"].strip(), f'meta[{attr}=\"{val}\"]'\n",
    "    return None, None\n",
    "\n",
    "def parse_iso_like(iso_str):\n",
    "    if not iso_str:\n",
    "        return None\n",
    "    s = iso_str.replace(\"Z\", \"+00:00\")\n",
    "    dt = None\n",
    "    try:\n",
    "        dt = datetime.fromisoformat(s)\n",
    "    except Exception:\n",
    "        for fmt in (\n",
    "            \"%Y-%m-%dT%H:%M:%S%z\",\n",
    "            \"%Y-%m-%dT%H:%M:%S.%f%z\",\n",
    "            \"%Y-%m-%d %H:%M:%S%z\",\n",
    "            \"%Y-%m-%dT%H:%M\",\n",
    "            \"%Y-%m-%d %H:%M:%S\",\n",
    "        ):\n",
    "            try:\n",
    "                dt = datetime.strptime(s, fmt)\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "    if dt and dt.tzinfo is None:\n",
    "        from zoneinfo import ZoneInfo\n",
    "        dt = dt.replace(tzinfo=ZoneInfo(SGT_TZ))\n",
    "    return dt\n",
    "\n",
    "def to_sgt(dt):\n",
    "    from zoneinfo import ZoneInfo\n",
    "    return dt.astimezone(ZoneInfo(SGT_TZ))\n",
    "\n",
    "def within_window(dt):\n",
    "    return START_DATE <= dt.date() <= END_DATE\n",
    "\n",
    "def fetch_article_soup(url):\n",
    "    r = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    return BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "def extract_title(soup):\n",
    "    h1 = soup.find(\"h1\")\n",
    "    return h1.get_text(strip=True) if h1 else \"\"\n",
    "\n",
    "# ================= Selenium setup (fixed) =================\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless=new\")  # comment this if you want to see the browser\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--window-size=1280,2000\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "# ✅ Correct way: use service= and options=\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "service = Service(ChromeDriverManager().install())\n",
    "\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "driver.get(LISTING_URL)\n",
    "\n",
    "# Wait for first batch of cards to appear\n",
    "try:\n",
    "    WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href]\"))\n",
    "    )\n",
    "except Exception:\n",
    "    driver.quit()\n",
    "    raise SystemExit(\"Failed to load the listing page or find anchors.\")\n",
    "\n",
    "rows = []\n",
    "seen_links = set()\n",
    "\n",
    "# For early-stopping heuristics\n",
    "global_oldest_seen = None\n",
    "batches_since_last_new_in_window = 0\n",
    "\n",
    "for batch in range(1, MAX_SCROLL_BATCHES + 1):\n",
    "    # Scroll to bottom to trigger lazy load/infinite scroll\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(SCROLL_PAUSE_SEC)\n",
    "    time.sleep(NEW_CARDS_SETTLE_SEC)\n",
    "\n",
    "    # Collect anchors currently in DOM\n",
    "    anchors = driver.find_elements(By.CSS_SELECTOR, \"a[href]\")\n",
    "    hrefs = set()\n",
    "    for a in anchors:\n",
    "        try:\n",
    "            href = a.get_attribute(\"href\")\n",
    "        except Exception:\n",
    "            continue\n",
    "        if not href:\n",
    "            continue\n",
    "        if not same_domain(href):\n",
    "            continue\n",
    "        hrefs.add(href)\n",
    "\n",
    "    # Process only NEW links (dedup)\n",
    "    new_links = sorted([u for u in hrefs if u not in seen_links])\n",
    "    if not new_links and batches_since_last_new_in_window >= 2:\n",
    "        print(f\"\\nNo new links for 2 batches. Stopping at batch {batch}.\")\n",
    "        break\n",
    "\n",
    "    kept_this_batch = 0\n",
    "    checked_meta_this_batch = 0\n",
    "    drop_before_this_batch = 0\n",
    "    drop_after_this_batch = 0\n",
    "    no_meta_this_batch = 0\n",
    "    errors_this_batch = 0\n",
    "    oldest_this_batch = None\n",
    "\n",
    "    for u in new_links:\n",
    "        seen_links.add(u)\n",
    "\n",
    "        # Quick cheap skip for obvious section roots\n",
    "        path = urlparse(u).path.rstrip(\"/\")\n",
    "        if path in {\"\", \"/\", \"/property\", \"/property/residential\", \"/property/commercial-industrial\"}:\n",
    "            continue\n",
    "\n",
    "        # Fetch article page and check for published_time meta\n",
    "        try:\n",
    "            art = fetch_article_soup(u)\n",
    "        except Exception:\n",
    "            errors_this_batch += 1\n",
    "            continue\n",
    "\n",
    "        iso, src = find_published_meta(art)\n",
    "        if not iso:\n",
    "            no_meta_this_batch += 1\n",
    "            continue\n",
    "\n",
    "        checked_meta_this_batch += 1\n",
    "        dt = parse_iso_like(iso)\n",
    "        if not dt:\n",
    "            # meta exists but unparsable; treat as no-date\n",
    "            continue\n",
    "\n",
    "        sgt_dt = to_sgt(dt)\n",
    "        if (oldest_this_batch is None) or (sgt_dt.date() < oldest_this_batch):\n",
    "            oldest_this_batch = sgt_dt.date()\n",
    "        if (global_oldest_seen is None) or (sgt_dt.date() < global_oldest_seen):\n",
    "            global_oldest_seen = sgt_dt.date()\n",
    "\n",
    "        if within_window(sgt_dt):\n",
    "            title = extract_title(art)\n",
    "            rows.append({\n",
    "                \"title\": title,\n",
    "                \"url\": u,\n",
    "                \"published_sgt\": sgt_dt.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "            })\n",
    "            kept_this_batch += 1\n",
    "        elif sgt_dt.date() < START_DATE:\n",
    "            drop_before_this_batch += 1\n",
    "        else:\n",
    "            drop_after_this_batch += 1\n",
    "\n",
    "        time.sleep(PAUSE_BETWEEN_ARTICLE_REQUESTS)\n",
    "\n",
    "    # ---- Batch summary (what you asked to monitor) ----\n",
    "    print(f\"\\n--- Batch {batch} summary ---\")\n",
    "    print(f\"New links discovered:      {len(new_links)}\")\n",
    "    print(f\"Meta-backed links checked: {checked_meta_this_batch}\")\n",
    "    print(f\"Kept (in window):          {kept_this_batch}\")\n",
    "    print(f\"Dropped before window:     {drop_before_this_batch}\")\n",
    "    print(f\"Dropped after window:      {drop_after_this_batch}\")\n",
    "    print(f\"No published_time meta:    {no_meta_this_batch}\")\n",
    "    print(f\"Errors fetching articles:  {errors_this_batch}\")\n",
    "    print(f\"Oldest date this batch:    {oldest_this_batch or '(none)'}\")\n",
    "    print(f\"Global oldest seen:        {global_oldest_seen or '(none)'}\")\n",
    "\n",
    "    # early-stop rule:\n",
    "    # - if we’ve seen any dates and the global oldest is earlier than START_DATE,\n",
    "    #   and we didn’t keep anything in the last 2 batches, stop.\n",
    "    if kept_this_batch > 0:\n",
    "        batches_since_last_new_in_window = 0\n",
    "    else:\n",
    "        batches_since_last_new_in_window += 1\n",
    "\n",
    "    if (global_oldest_seen is not None) and (global_oldest_seen < START_DATE) and (batches_since_last_new_in_window >= 2):\n",
    "        print(\"\\nReached older-than-start dates and no new in-window items for 2 batches. Stopping.\")\n",
    "        break\n",
    "\n",
    "# Tidy up Selenium\n",
    "driver.quit()\n",
    "\n",
    "# ===== Output =====\n",
    "rows.sort(key=lambda r: r[\"published_sgt\"])\n",
    "with open(OUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"title\", \"url\", \"published_sgt\"])\n",
    "    w.writeheader()\n",
    "    w.writerows(rows)\n",
    "\n",
    "print(\"\\n=== Overall ===\")\n",
    "print(f\"Total kept (in window): {len(rows)}\")\n",
    "print(f\"Saved to: {OUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a89c32-2ed9-4738-b732-d297129c4468",
   "metadata": {},
   "source": [
    "# scrape articles from output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee1fe058-01b2-41c3-9842-b60d143734e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= CONFIG =======\n",
    "INPUT_CSV = \"Output/bt_residential_2025-01-01_to_2025-09-30.csv\"  # must contain a column named 'url'\n",
    "OUTPUT_XLSX = \"Output/bt_articles_with_content.xlsx\"\n",
    "\n",
    "# If True, launch Chrome with Selenium so you can log in to BT once.\n",
    "USE_SELENIUM_AUTH = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44b01b2c-f86d-4cbc-b008-9137307ebf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A Chrome window has opened. Please log in to The Business Times with your subscription.\n",
      "Once you can access a paywalled article in that window, return here and press Enter to continue.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter after logging in successfully...  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/58] Fetching: /property/residential/river-valley-apartments-sale-s56-million\n",
      "    -> 2732 characters\n",
      "[2/58] Fetching: /property/residential/good-class-bungalow-sales-volume-2025-expected-match-if-not-exceed-last-years-tally\n",
      "    -> 8545 characters\n",
      "[3/58] Fetching: /property/residential/dont-spare-asset-rich-cash-poor-private-homeowners-paying-higher-property-taxes\n",
      "    -> 6448 characters\n",
      "[4/58] Fetching: /property/residential/measured-bids-tengah-housing-plot-bullish-play-dairy-farm-site-developers-stay-conservative\n",
      "    -> 6484 characters\n",
      "[5/58] Fetching: /property/hdb-supply-19600-bto-flats-2025-amid-continued-rise-resale-prices\n",
      "    -> 3922 characters\n",
      "[6/58] Fetching: /property/residential/jadescape-owner-sells-unit-s4-4-million-profit-after-5-years-topping-q4-resale-gains\n",
      "    -> 5595 characters\n",
      "[7/58] Fetching: /property/residential/hdb-resale-prices-climb-9-7-2024-rise-double-2023\n",
      "    -> 6093 characters\n",
      "[8/58] Fetching: /property/residential/ditch-vip-inside-track-new-condo-launches\n",
      "    -> 6387 characters\n",
      "[9/58] Fetching: /property/residential/two-park-nova-units-sold-january-near-record-prices-one-penthouse-s6593-psf\n",
      "    -> 4675 characters\n",
      "[10/58] Fetching: /opinion-features/budget-wishes-housing-raise-income-ceiling-and-drop-mortgage-servicing-ratio-executive-condos\n",
      "    -> 653 characters\n",
      "[11/58] Fetching: /property/residential/mcl-land-csc-land-launch-clementi-condo-elta-prices-start-about-s2200-psf\n",
      "    -> 2483 characters\n",
      "[12/58] Fetching: /property/residential/uol-singland-capitaland-development-market-tampines-mega-project-parktown-residence-prices-start-s1\n",
      "    -> 2965 characters\n",
      "[13/58] Fetching: /opinion-features/shifting-housing-demand-singapore-johor-could-offer-room-ease-absd-rates\n",
      "    -> 6758 characters\n",
      "[14/58] Fetching: /opinion-features/couples-buying-hdb-bto-flats-should-register-ownership-names-both-partners\n",
      "    -> 5351 characters\n",
      "[15/58] Fetching: /opinion-features/s2-5-million-affordable-price-tag-new-leasehold-3-bedroom-condo\n",
      "    -> 6550 characters\n",
      "[16/58] Fetching: /property/boutique-agency-vesper-homes-positive-singapore-property-amid-geopolitical-unease\n",
      "    -> 5210 characters\n",
      "[17/58] Fetching: /opinion-features/feeling-fomo-new-condo-market-mind-choppy-seas-higher-trade-tariffs\n",
      "    -> 6370 characters\n",
      "[18/58] Fetching: /opinion-features/parktown-residences-sizzling-sales-perks-and-perils-buying-large-integrated-developments\n",
      "    -> 6894 characters\n",
      "[19/58] Fetching: /property/number-singapore-residents-living-private-condos-doubles-over-last-15-years\n",
      "    -> 9553 characters\n",
      "[20/58] Fetching: /opinion-features/who-wants-be-property-agent-rewards-could-outweigh-risks\n",
      "    -> 6408 characters\n",
      "[21/58] Fetching: /opinion-features/let-large-families-buy-new-hdb-3gen-flats-and-enjoy-priority-4-and-5-bedder-exec-condos\n",
      "    -> 6283 characters\n",
      "[22/58] Fetching: /property/teo-hock-sengs-old-holland-road-bungalow-auction-s36-million-opening-price\n",
      "    -> 5049 characters\n",
      "[23/58] Fetching: /property/elias-greens-collective-sale-closes-no-bids\n",
      "    -> 1642 characters\n",
      "[24/58] Fetching: /property/s16-million-deal-leedon-residence-tops-q1-gains-seller-reaping-s4-million-profit\n",
      "    -> 4827 characters\n",
      "[25/58] Fetching: /property/mortgagee-listings-climb-q1-interest-rate-strains-hit-market-knight-frank\n",
      "    -> 3319 characters\n",
      "[26/58] Fetching: /opinion-features/trade-tariffs-may-remove-fomo-among-new-condo-buyers-open-way-relax-absd\n",
      "    -> 6301 characters\n",
      "[27/58] Fetching: /property/ura-puts-second-chuan-grove-site-sale-firm-bids-expected-amid-steady-demand-area\n",
      "    -> 3301 characters\n",
      "[28/58] Fetching: /property/lornie-road-bungalow-late-property-developer-market-s82-million-guide-price\n",
      "    -> 2877 characters\n",
      "[29/58] Fetching: /property/hillcrest-arcadias-s920-million-en-bloc-tender-closes-no-bids\n",
      "    -> 2765 characters\n",
      "[30/58] Fetching: /property/peirce-hill-bungalow-owned-wife-3acs-kyle-davies-sells-s37-million\n",
      "    -> 3106 characters\n",
      "[31/58] Fetching: /property/cdl-places-s1132-psf-ppr-top-bid-plot-next-lakeside-mrt-station\n",
      "    -> 5029 characters\n",
      "[32/58] Fetching: /opinion-features/will-deglobalisation-and-business-uncertainty-hurt-buying-power-good-class-bungalow-properties\n",
      "    -> 6456 characters\n",
      "[33/58] Fetching: /property/mnd-release-land-confirmed-list-4725-private-housing-units-h2-2025-down-6-5030-units-h1\n",
      "    -> 7975 characters\n",
      "[34/58] Fetching: /opinion-features/consider-ideas-such-hdb-homes-wellness-resort-tyersall-avenue-near-singapore-botanic-gardens\n",
      "    -> 5977 characters\n",
      "[35/58] Fetching: /opinion-features/latest-gls-programme-designed-draw-developers-back-state-tenders\n",
      "    -> 8276 characters\n",
      "[36/58] Fetching: /opinion-features/see-positives-having-choice-hdb-resale-flats-transact-over-million-dollars-each\n",
      "    -> 6149 characters\n",
      "[37/58] Fetching: /opinion-features/draft-master-plan-2025-builds-capacity-its-success-hinges-developers-stepping-skilled-migrants\n",
      "    -> 6130 characters\n",
      "[38/58] Fetching: /property/new-private-homes-planned-paterson-newton-areas\n",
      "    -> 7118 characters\n",
      "[39/58] Fetching: /property/frasers-sekisui-csc-land-consortium-tops-nine-bids-dunearn-road-site-s1410-psf-ppr-bid\n",
      "    -> 5357 characters\n",
      "[40/58] Fetching: /property/guocolands-dora-chng-flex-designs-and-biodiversity-lure-new-condo-buyers\n",
      "    -> 4603 characters\n",
      "[41/58] Fetching: /opinion-features/columns/tighten-en-bloc-rules-encourage-reuse-so-more-private-housing-blocks-stand-over-50-years\n",
      "    -> 6683 characters\n",
      "[42/58] Fetching: /opinion-features/low-mortgage-rates-help-be-cautious-pursuing-condo-dreams-economic-outlook-weakens\n",
      "    -> 685 characters\n",
      "[43/58] Fetching: /opinion-features/getting-real-good-class-bungalow-prices\n",
      "    -> 14066 characters\n",
      "[44/58] Fetching: /opinion-features/should-capital-gains-tax-replace-sellers-stamp-duty-homes\n",
      "    -> 6289 characters\n",
      "[45/58] Fetching: /opinion-features/dont-count-out-cbd-homes\n",
      "    -> 6020 characters\n",
      "[46/58] Fetching: /property/firm-take-upperhouse-and-robertson-opus-prices-averaging-about-s3350-psf\n",
      "    -> 6592 characters\n",
      "[47/58] Fetching: /property/hotel-miramar-singapore-close-sealing-sale-under-s200-million\n",
      "    -> 5724 characters\n",
      "[48/58] Fetching: /companies-markets/sim-lian-places-bullish-top-bid-s1432-psf-ppr-holland-link-site\n",
      "    -> 5376 characters\n",
      "[49/58] Fetching: /opinion-features/new-400-sq-ft-condo-home-worth-buying\n",
      "    -> 6626 characters\n",
      "[50/58] Fetching: /opinion-features/time-raise-income-ceilings-buyers-new-hdb-and-ec-units-and-have-bi-annual-fixed-increases\n",
      "    -> 6576 characters\n",
      "[51/58] Fetching: /property/land-betterment-charges-commercial-residential-and-industrial-uses-go-0-1-1-6-average\n",
      "    -> 5410 characters\n",
      "[52/58] Fetching: /property/real-estate-players-split-whether-sellers-stamp-duty-hikes-will-curb-speculation-nus-poll\n",
      "    -> 3538 characters\n",
      "[53/58] Fetching: /property/tycoon-couple-gordon-and-celine-tangs-son-picks-second-avenue-gcb-s53-million\n",
      "    -> 4257 characters\n",
      "[54/58] Fetching: /property/sing-holdings-sunway-jv-tops-bids-second-chuan-grove-site-eyes-amalgamation\n",
      "    -> 5746 characters\n",
      "[55/58] Fetching: /property/evia-gamuda-ho-lee-consortium-places-top-bid-s1-01-billion-or-s980-psf-ppr-yishun-site\n",
      "    -> 4858 characters\n",
      "[56/58] Fetching: /opinion-features/insiders-should-join-back-instead-front-queue-new-condo-launches\n",
      "    -> 5851 characters\n",
      "[57/58] Fetching: /opinion-features/could-hdb-offer-buy-old-flats-do-not-undergo-vers\n",
      "    -> 6096 characters\n",
      "[58/58] Fetching: /property/guocoland-launch-faber-residence-prices-starting-s1995-psf\n",
      "    -> 5098 characters\n",
      "\n",
      "Saved 58 rows to Output/bt_articles_with_content.xlsx\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import time\n",
    "import html\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Politeness\n",
    "REQUEST_TIMEOUT = 25\n",
    "PAUSE_BETWEEN_REQUESTS = 0.5  # seconds\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Some phrases/containers we typically want to skip if they appear in-paragraph\n",
    "STOP_PHRASES = {\n",
    "    \"Get the BT app\", \"Get WhatsApp alerts\", \"Also read:\", \"Related:\", \"More on this topic\",\n",
    "    \"Have a news tip?\", \"Sign up\", \"Subscribe to\", \"Unlimited access\", \"Already a subscriber\"\n",
    "}\n",
    "\n",
    "\n",
    "# ======= REQUESTS MODE (no login) =======\n",
    "def fetch_html_requests(url: str) -> str | None:\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)\n",
    "        r.raise_for_status()\n",
    "        # Best-effort encoding normalisation\n",
    "        if not r.encoding:\n",
    "            r.encoding = \"utf-8\"\n",
    "        return r.text\n",
    "    except Exception as e:\n",
    "        print(f\"  [requests] fetch failed for {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ======= SELENIUM MODE (login once, use your subscription) =======\n",
    "driver = None\n",
    "def init_selenium():\n",
    "    global driver\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "    chrome_options = Options()\n",
    "    # Comment out the next line if you want to see Chrome UI (recommended for login)\n",
    "    # chrome_options.add_argument(\"--headless=new\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1280,2000\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    # Open BT site so you can log in with your subscription\n",
    "    driver.get(\"https://www.businesstimes.com.sg/\")\n",
    "    print(\"\\nA Chrome window has opened. Please log in to The Business Times with your subscription.\")\n",
    "    print(\"Once you can access a paywalled article in that window, return here and press Enter to continue.\")\n",
    "    input(\"Press Enter after logging in successfully... \")\n",
    "\n",
    "\n",
    "def fetch_html_selenium(url: str) -> str | None:\n",
    "    global driver\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        # Give the page some time to render (adjust if needed)\n",
    "        time.sleep(2.0)\n",
    "        return driver.page_source\n",
    "    except Exception as e:\n",
    "        print(f\"  [selenium] fetch failed for {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ======= CONTENT EXTRACTION =======\n",
    "def parse_jsonld_article_body(soup: BeautifulSoup) -> str | None:\n",
    "    \"\"\"\n",
    "    Try to get articleBody from JSON-LD if present.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for tag in soup.find_all(\"script\", type=\"application/ld+json\"):\n",
    "            # Some pages have multiple JSON-LD blocks; parse safely\n",
    "            txt = tag.string or tag.get_text()\n",
    "            if not txt:\n",
    "                continue\n",
    "            data = json.loads(txt.strip())\n",
    "            # Could be a list or a dict\n",
    "            candidates = data if isinstance(data, list) else [data]\n",
    "            for obj in candidates:\n",
    "                if not isinstance(obj, dict):\n",
    "                    continue\n",
    "                t = (obj.get(\"@type\") or \"\").lower()\n",
    "                if \"article\" in t or \"newsarticle\" in t or \"reportage\" in t:\n",
    "                    body = obj.get(\"articleBody\")\n",
    "                    if body:\n",
    "                        return html.unescape(body).strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_article_text_from_dom(soup: BeautifulSoup) -> str:\n",
    "    \"\"\"\n",
    "    Fallback: collect paragraphs from <article> or main content container.\n",
    "    We avoid obvious non-article blocks (aside, nav, footer).\n",
    "    \"\"\"\n",
    "    # Prefer <article> tag if available\n",
    "    root = soup.find(\"article\")\n",
    "    if root is None:\n",
    "        # fall back to the biggest content-ish container\n",
    "        for sel in [\"main\", \"section\", \"div[role='main']\", \"div.content\", \"div.article\"]:\n",
    "            root = soup.select_one(sel)\n",
    "            if root:\n",
    "                break\n",
    "    if root is None:\n",
    "        root = soup  # last resort: whole document\n",
    "\n",
    "    # gather <p> that look like article paragraphs\n",
    "    paras = []\n",
    "    for p in root.find_all(\"p\"):\n",
    "        # Skip empty, nav, footer crumbs\n",
    "        txt = p.get_text(\" \", strip=True)\n",
    "        if not txt:\n",
    "            continue\n",
    "        # Skip lines that are clearly utility / promotion\n",
    "        if any(stop in txt for stop in STOP_PHRASES):\n",
    "            continue\n",
    "        paras.append(txt)\n",
    "\n",
    "    # If nothing found under root, try all <p> (very last resort)\n",
    "    if not paras:\n",
    "        for p in soup.find_all(\"p\"):\n",
    "            txt = p.get_text(\" \", strip=True)\n",
    "            if txt and not any(stop in txt for stop in STOP_PHRASES):\n",
    "                paras.append(txt)\n",
    "\n",
    "    # Join with blank lines to preserve paragraph breaks a bit\n",
    "    content = \"\\n\\n\".join(paras).strip()\n",
    "    # Clean stray HTML entities\n",
    "    content = html.unescape(content)\n",
    "    return content\n",
    "\n",
    "\n",
    "def extract_article_content(html_text: str) -> str:\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "\n",
    "    # First try JSON-LD articleBody\n",
    "    body = parse_jsonld_article_body(soup)\n",
    "    if body and len(body) > 80:  # sanity check length\n",
    "        return body.strip()\n",
    "\n",
    "    # Fallback: DOM paragraphs\n",
    "    return extract_article_text_from_dom(soup)\n",
    "\n",
    "\n",
    "# ======= MAIN =======\n",
    "def main():\n",
    "    global driver\n",
    "\n",
    "    # Load URLs\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    # Normalise column name\n",
    "    if \"url\" not in df.columns:\n",
    "        # try case-insensitive\n",
    "        for c in df.columns:\n",
    "            if c.lower().strip() == \"url\":\n",
    "                df.rename(columns={c: \"url\"}, inplace=True)\n",
    "                break\n",
    "    if \"url\" not in df.columns:\n",
    "        raise SystemExit(\"Input CSV must contain a 'url' column.\")\n",
    "\n",
    "    if USE_SELENIUM_AUTH:\n",
    "        init_selenium()\n",
    "\n",
    "    contents = []\n",
    "    char_counts = []\n",
    "\n",
    "    for i, url in enumerate(df[\"url\"], start=1):\n",
    "        if not isinstance(url, str) or not url.strip():\n",
    "            contents.append(\"\")\n",
    "            char_counts.append(0)\n",
    "            continue\n",
    "\n",
    "        print(f\"[{i}/{len(df)}] Fetching: {urlparse(url).path}\")\n",
    "        time.sleep(PAUSE_BETWEEN_REQUESTS)\n",
    "\n",
    "        # Fetch HTML (requests or selenium)\n",
    "        html_text = fetch_html_selenium(url) if USE_SELENIUM_AUTH else fetch_html_requests(url)\n",
    "        if not html_text:\n",
    "            print(\"    -> fetch failed; leaving content empty\")\n",
    "            contents.append(\"\")\n",
    "            char_counts.append(0)\n",
    "            continue\n",
    "\n",
    "        # Extract content\n",
    "        content = extract_article_content(html_text)\n",
    "        n_chars = len(content)\n",
    "\n",
    "        print(f\"    -> {n_chars} characters\")\n",
    "        contents.append(content)\n",
    "        char_counts.append(n_chars)\n",
    "\n",
    "    # Add columns and save to Excel\n",
    "    df[\"content\"] = contents\n",
    "    df[\"content_chars\"] = char_counts\n",
    "\n",
    "    # Make sure output folder exists\n",
    "    Path(OUTPUT_XLSX).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_excel(OUTPUT_XLSX, index=False)\n",
    "    print(f\"\\nSaved {len(df)} rows to {OUTPUT_XLSX}\")\n",
    "\n",
    "    # Clean up Selenium\n",
    "    if USE_SELENIUM_AUTH and driver is not None:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57935127-74ba-4c5a-a83a-beab5938261c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
