{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5378b9ee-c347-44b5-b7be-5d14baafb8ec",
   "metadata": {},
   "source": [
    "# scrape list of CNA articles on one CNA topic website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f67b343-5ec6-4dc5-adfd-67ac357036ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# CONFIG\n",
    "# --------------------------\n",
    "TOPIC_URL = \"https://www.channelnewsasia.com/topic/housing-and-development-board\"\n",
    "START_DATE = date(2025, 1, 1)   # inclusive\n",
    "END_DATE   = date(2025, 9, 30)  # inclusive\n",
    "OUTPUT_CSV = \"Output/cna_housing_and_development_board_20250101_20250930.csv\"\n",
    "\n",
    "MAX_PAGES = 10  # maximum number of pages to attempt (we now start at page=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "524e9ad3-3e37-4303-89a9-d7ff10c94426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CNA Housing articles from 2025-01-01 to 2025-09-30 ...\n",
      "\n",
      "[Page 0] https://www.channelnewsasia.com/topic/housing-and-development-board?page=0\n",
      "Found 81 candidate links; fetching articles...\n",
      "Kept in range on this page: 12 | Older-than-start on this page: 0\n",
      "\n",
      "[Page 1] https://www.channelnewsasia.com/topic/housing-and-development-board?page=1\n",
      "Found 74 candidate links; fetching articles...\n",
      "Kept in range on this page: 9 | Older-than-start on this page: 0\n",
      "\n",
      "[Page 2] https://www.channelnewsasia.com/topic/housing-and-development-board?page=2\n",
      "Found 81 candidate links; fetching articles...\n",
      "Kept in range on this page: 4 | Older-than-start on this page: 11\n",
      "\n",
      "[Page 3] https://www.channelnewsasia.com/topic/housing-and-development-board?page=3\n",
      "Found 78 candidate links; fetching articles...\n",
      "Kept in range on this page: 0 | Older-than-start on this page: 12\n",
      "Reached mostly items older than start date; stopping pagination.\n",
      "\n",
      "Done. Saved 25 rows to Output/cna_housing_and_development_board_20250101_20250930.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from datetime import date, datetime, timezone\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Polite crawling\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "REQUEST_TIMEOUT = 20\n",
    "SLEEP_BETWEEN_REQUESTS = 1.0  # seconds\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# HELPERS\n",
    "# --------------------------\n",
    "def is_article_url(url: str) -> bool:\n",
    "    u = urlparse(url)\n",
    "    if u.netloc not in {\"www.channelnewsasia.com\", \"channelnewsasia.com\"}:\n",
    "        return False\n",
    "    path = u.path.lower()\n",
    "\n",
    "    # Exclude clear non-article sections\n",
    "    if path.startswith(\"/watch\") or \"/watch/\" in path:\n",
    "        return False\n",
    "    if \"/podcast\" in path or \"/podcasts\" in path or path.startswith(\"/listen\"):\n",
    "        return False\n",
    "    if path.startswith(\"/profile\") or path.startswith(\"/rss\") or path.startswith(\"/weather\"):\n",
    "        return False\n",
    "    if path == \"/\":\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "ISO_DT_RE = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}\")\n",
    "\n",
    "def parse_publish_datetime_from_article(html: str) -> datetime | None:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # 1) JSON-LD blocks\n",
    "    for script in soup.find_all(\"script\", type=\"application/ld+json\"):\n",
    "        try:\n",
    "            data = json.loads(script.string or \"\")\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        candidates = data if isinstance(data, list) else [data]\n",
    "        for obj in candidates:\n",
    "            if not isinstance(obj, dict):\n",
    "                continue\n",
    "            typ = obj.get(\"@type\") or obj.get(\"@context\")\n",
    "            if (isinstance(typ, str) and \"Article\" in typ) or obj.get(\"headline\") or obj.get(\"datePublished\"):\n",
    "                dt = obj.get(\"datePublished\") or obj.get(\"dateModified\")\n",
    "                if isinstance(dt, str) and ISO_DT_RE.match(dt):\n",
    "                    try:\n",
    "                        dt_parsed = datetime.fromisoformat(dt.replace(\"Z\", \"+00:00\"))\n",
    "                        SGT = timezone(timedelta(hours=8))\n",
    "                        return dt_parsed.astimezone(SGT)\n",
    "                        \n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "    # 2) Meta tags\n",
    "    meta_names = [\n",
    "        (\"meta\", {\"property\": \"article:published_time\"}),\n",
    "        (\"meta\", {\"name\": \"pubdate\"}),\n",
    "        (\"meta\", {\"name\": \"publish-date\"}),\n",
    "        (\"meta\", {\"name\": \"date\"}),\n",
    "        (\"meta\", {\"itemprop\": \"datePublished\"}),\n",
    "        (\"meta\", {\"property\": \"og:pubdate\"}),\n",
    "        (\"meta\", {\"name\": \"parsely-pub-date\"}),\n",
    "        (\"meta\", {\"name\": \"cXenseParse:recs:publishtime\"}),\n",
    "    ]\n",
    "    for tag, attrs in meta_names:\n",
    "        el = soup.find(tag, attrs=attrs)\n",
    "        if el and el.get(\"content\"):\n",
    "            dt = el[\"content\"].strip()\n",
    "            if ISO_DT_RE.match(dt):\n",
    "                try:\n",
    "                    dt_parsed = datetime.fromisoformat(dt.replace(\"Z\", \"+00:00\"))\n",
    "                    SGT = timezone(timedelta(hours=8))\n",
    "                    return dt_parsed.astimezone(SGT)\n",
    "\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    # 3) <time> tags\n",
    "    for t in soup.find_all(\"time\"):\n",
    "        dt = (t.get(\"datetime\") or t.get(\"content\") or \"\").strip()\n",
    "        if ISO_DT_RE.match(dt):\n",
    "            try:\n",
    "                dt_parsed = datetime.fromisoformat(dt.replace(\"Z\", \"+00:00\"))\n",
    "                SGT = timezone(timedelta(hours=8))\n",
    "                return dt_parsed.astimezone(SGT)\n",
    "\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def fetch(url: str) -> str | None:\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)\n",
    "        r.encoding = \"utf-8\"\n",
    "        r.raise_for_status()\n",
    "        return r.text\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] GET failed: {url} | {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def within_range(dt: datetime) -> bool:\n",
    "    d = dt.astimezone(timezone.utc).date()\n",
    "    return START_DATE <= d <= END_DATE\n",
    "\n",
    "\n",
    "def date_is_before_start(dt: datetime) -> bool:\n",
    "    return dt.astimezone(timezone.utc).date() < START_DATE\n",
    "\n",
    "\n",
    "def extract_listing_links(topic_html: str) -> list[str]:\n",
    "    soup = BeautifulSoup(topic_html, \"html.parser\")\n",
    "    hrefs = set()\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if href.startswith(\"#\"):\n",
    "            continue\n",
    "        abs_url = urljoin(TOPIC_URL, href)\n",
    "        if is_article_url(abs_url):\n",
    "            hrefs.add(abs_url)\n",
    "    return sorted(hrefs)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# MAIN\n",
    "# --------------------------\n",
    "def main():\n",
    "    print(f\"Scraping CNA Housing articles from {START_DATE} to {END_DATE} ...\")\n",
    "    rows = []\n",
    "    seen_urls = set()\n",
    "\n",
    "    reached_older_than_start = False\n",
    "\n",
    "    # Start from page=0 now\n",
    "    for page in range(0, MAX_PAGES):\n",
    "        page_url = f\"{TOPIC_URL}?page={page}\"\n",
    "        print(f\"\\n[Page {page}] {page_url}\")\n",
    "        html = fetch(page_url)\n",
    "        if not html:\n",
    "            print(\"No HTML returned; stopping.\")\n",
    "            break\n",
    "\n",
    "        links = extract_listing_links(html)\n",
    "        if not links:\n",
    "            print(\"No candidate links found on page; stopping.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Found {len(links)} candidate links; fetching articles...\")\n",
    "        page_old_count = 0\n",
    "        page_in_range = 0\n",
    "\n",
    "        for article_url in links:\n",
    "            if article_url in seen_urls:\n",
    "                continue\n",
    "            seen_urls.add(article_url)\n",
    "\n",
    "            time.sleep(SLEEP_BETWEEN_REQUESTS)\n",
    "            article_html = fetch(article_url)\n",
    "            if not article_html:\n",
    "                continue\n",
    "\n",
    "            # Parse date\n",
    "            dt = parse_publish_datetime_from_article(article_html)\n",
    "            if not dt:\n",
    "                continue\n",
    "\n",
    "            if within_range(dt):\n",
    "                soup = BeautifulSoup(article_html, \"html.parser\")\n",
    "\n",
    "                # Title\n",
    "                meta_title = soup.find(\"meta\", property=\"og:title\")\n",
    "                if meta_title and meta_title.get(\"content\"):\n",
    "                    title = meta_title[\"content\"].strip()\n",
    "                else:\n",
    "                    t = soup.find(\"title\")\n",
    "                    title = t.text.strip() if t else \"\"\n",
    "\n",
    "                rows.append({\n",
    "                    \"title\": title,\n",
    "                    \"url\": article_url,\n",
    "                    \"published_utc\": dt.isoformat(),\n",
    "                })\n",
    "                page_in_range += 1\n",
    "            elif date_is_before_start(dt):\n",
    "                page_old_count += 1\n",
    "\n",
    "        print(f\"Kept in range on this page: {page_in_range} | Older-than-start on this page: {page_old_count}\")\n",
    "\n",
    "        if page_in_range == 0 and page_old_count >= 10:\n",
    "            reached_older_than_start = True\n",
    "            print(\"Reached mostly items older than start date; stopping pagination.\")\n",
    "            break\n",
    "\n",
    "    # Deduplicate by URL and sort by date\n",
    "    dedup = {r[\"url\"]: r for r in rows}\n",
    "    final_rows = sorted(dedup.values(), key=lambda r: r[\"published_utc\"])\n",
    "\n",
    "    # Write CSV (only the requested columns)\n",
    "    fieldnames = [\"title\", \"url\", \"published_utc\"]\n",
    "    with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(final_rows)\n",
    "\n",
    "    print(f\"\\nDone. Saved {len(final_rows)} rows to {OUTPUT_CSV}\")\n",
    "    if not reached_older_than_start:\n",
    "        print(\"Note: You may increase MAX_PAGES if you think there are more pages within range.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e0731c-4c01-4635-ae7b-d124234ed402",
   "metadata": {},
   "source": [
    "# scrape contents of articles from output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c8bf7a7-5d2c-49e5-8084-00bc3904bea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# CONFIG\n",
    "# --------------------------\n",
    "INPUT_CSV = \"Output/cna_housing_and_development_board_20250101_20250930.csv\"  # CSV with at least a 'url' column\n",
    "OUTPUT_XLSX = \"Output/articles_cna_housing_and_development_board_20250101_20250930.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46d6c549-7125-4a1d-aada-a4534117628a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading URLs from: Output/cna_housing_and_development_board_20250101_20250930.csv\n",
      "Found 25 URL(s).\n",
      "[1/25] Scraping: https://www.channelnewsasia.com/singapore/worksite-death-tengah-hdb-plantation-edge-bto-indian-national-construction-workplace-safety-mom-4843271\n",
      "[2/25] Scraping: https://www.channelnewsasia.com/singapore/19600-bto-flats-launched-2025-desmond-lee-4859601\n",
      "[3/25] Scraping: https://www.channelnewsasia.com/singapore/singapore-not-averse-new-property-cooling-measures-desmond-lee-4860611\n",
      "[4/25] Scraping: https://www.channelnewsasia.com/singapore/mogulsg-property-listings-ai-driven-tool-home-buyers-4918046\n",
      "[5/25] Scraping: https://www.channelnewsasia.com/singapore/hdb-home-improvement-programme-29000-flats-4941461\n",
      "[6/25] Scraping: https://www.channelnewsasia.com/singapore/hdb-february-2025-bto-sales-exercise-tanjong-rhu-queenstown-4943201\n",
      "[7/25] Scraping: https://www.channelnewsasia.com/singapore/bto-new-flats-multi-agency-committee-support-residents-5061846\n",
      "[8/25] Scraping: https://www.channelnewsasia.com/singapore/keppel-club-hdb-bto-flats-october-exercise-toa-payoh-5131711\n",
      "[9/25] Scraping: https://www.channelnewsasia.com/singapore/worker-dies-hdb-yishun-chencharu-tipper-truck-driver-arrested-5151431\n",
      "[10/25] Scraping: https://www.channelnewsasia.com/singapore/hdb-resale-flat-prices-moderate-bto-supply-chee-hong-tat-5156511\n",
      "[11/25] Scraping: https://www.channelnewsasia.com/singapore/monthly-rental-voucher-pphs-hdb-flat-bedroom-bto-5184076\n",
      "[12/25] Scraping: https://www.channelnewsasia.com/singapore/ura-draft-master-plan-unveils-proposals-new-homes-orchard-and-newton-5200016\n",
      "[13/25] Scraping: https://www.channelnewsasia.com/singapore/hdb-resale-prices-up-09-second-quarter-lowest-2020-5212951\n",
      "[14/25] Scraping: https://www.channelnewsasia.com/singapore/singapore-sellers-stamp-duty-4-years-private-property-5219066\n",
      "[15/25] Scraping: https://www.channelnewsasia.com/singapore/property-analysts-limited-impact-sellers-stamp-duty-holding-period-5220046\n",
      "[16/25] Scraping: https://www.channelnewsasia.com/singapore/housing-demand-population-ura-master-plan-5224401\n",
      "[17/25] Scraping: https://www.channelnewsasia.com/singapore/clementi-bukit-panjang-bto-flats-hdb-july-sales-exercise-5238591\n",
      "[18/25] Scraping: https://www.channelnewsasia.com/singapore/hdb-july-2025-sales-exercise-prime-flats-5253306\n",
      "[19/25] Scraping: https://www.channelnewsasia.com/singapore/bto-flats-greater-southern-waterfront-mount-pleasant-october-hdb-5257796\n",
      "[20/25] Scraping: https://www.channelnewsasia.com/singapore/about-22000-applications-received-july-bto-sales-exercise-so-far-chee-hong-tat-5267171\n",
      "[21/25] Scraping: https://www.channelnewsasia.com/singapore/gp-clinic-tender-bartley-beacon-bridgepoint-health-price-quality-method-5271956\n",
      "[22/25] Scraping: https://www.channelnewsasia.com/singapore/bto-income-ceiling-singles-age-review-chee-hong-tat-5284381\n",
      "[23/25] Scraping: https://www.channelnewsasia.com/singapore/vers-sers-hip-chee-hong-tat-5284406\n",
      "[24/25] Scraping: https://www.channelnewsasia.com/singapore/securing-buy-in-residents-possible-challenge-vers-housing-experts-5288086\n",
      "[25/25] Scraping: https://www.channelnewsasia.com/singapore/white-flats-offered-first-mount-pleasant-housing-bto-project-october-5327776\n",
      "✅ Saved Excel file to: Output/articles_cna_housing_and_development_board_20250101_20250930.xlsx\n",
      "\n",
      "Done. Scraped 25 article(s).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Scrape CNA article pages (title, publish time in Singapore, cleaned text)\n",
    "from a CSV list of URLs and save results to an Excel (.xlsx) file.\n",
    "\n",
    "Updates in this version:\n",
    "- Stops scraping body once it encounters the phrase \"Sign up for our newsletters\"\n",
    "  (case-insensitive), so ad/housekeeping blocks at the end are excluded.\n",
    "- Adds a 'content_chars' column (character count) to help detect truncation.\n",
    "\n",
    "Input CSV must contain a 'url' column (case-insensitive). If not found,\n",
    "the first column is treated as URL.\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import Workbook\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "REQUEST_TIMEOUT = 20\n",
    "SLEEP_BETWEEN_REQUESTS = 0.6  # seconds, be polite\n",
    "\n",
    "SGT = timezone(timedelta(hours=8))\n",
    "ISO_DT_RE = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}\")\n",
    "\n",
    "# Phrase that signals start of housekeeping/ads; stop collecting at this\n",
    "STOP_MARKER = \"sign up for our newsletters\"\n",
    "\n",
    "# Boilerplate lines to drop\n",
    "_BOILERPLATE_PATTERNS = [\n",
    "    r\"Get CNA updates on WhatsApp\",\n",
    "    r\"Subscribe to our newsletter\",\n",
    "    r\"Related:?$\",\n",
    "    r\"Recommended(?: for you)?:?$\",\n",
    "    r\"READ:?$\",\n",
    "    r\"READ ALSO:?$\",\n",
    "    r\"Follow us on\",\n",
    "    r\"Read more:?$\",\n",
    "    r\"Get the CNA app\",\n",
    "    r\"Get WhatsApp alerts\",\n",
    "    r\"Also worth reading\",\n",
    "]\n",
    "_BOILERPLATE_RE = re.compile(\"|\".join(_BOILERPLATE_PATTERNS), re.IGNORECASE)\n",
    "\n",
    "# Heuristic content class hints\n",
    "_CONTENT_CLASS_RE = re.compile(\n",
    "    r\"(article|story|post).*(content|body)|\"\n",
    "    r\"(content|text|rich)(-|_)?(body|area|container)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# HELPERS\n",
    "# --------------------------\n",
    "def ensure_dirs(path: str):\n",
    "    d = os.path.dirname(path)\n",
    "    if d:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "\n",
    "def fetch(url: str) -> str | None:\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)\n",
    "        # Ensure correct decoding to prevent â€œ/â€™ garbling\n",
    "        r.encoding = \"utf-8\"\n",
    "        r.raise_for_status()\n",
    "        return r.text\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] GET failed: {url} | {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def _clean_text(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\xa0\", \" \", s)\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "    if _BOILERPLATE_RE.search(s):\n",
    "        return \"\"\n",
    "    return s\n",
    "\n",
    "\n",
    "def _maybe_fix_misencoded(text: str) -> str:\n",
    "    \"\"\"\n",
    "    If viewer later shows â€œ/â€™ etc it’s usually Excel-view issue,\n",
    "    but if source is double-decoded, this can help.\n",
    "    \"\"\"\n",
    "    if \"â€\" in text or \"â€™\" in text or \"â€“\" in text:\n",
    "        try:\n",
    "            return text.encode(\"latin1\").decode(\"utf-8\")\n",
    "        except Exception:\n",
    "            return text\n",
    "    return text\n",
    "\n",
    "\n",
    "def _truncate_after_marker(text: str) -> str:\n",
    "    \"\"\"Cut content at the stop marker phrase (case-insensitive), if present.\"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    idx = text.lower().find(STOP_MARKER)\n",
    "    return text[:idx].rstrip() if idx != -1 else text\n",
    "\n",
    "\n",
    "def parse_publish_datetime_sgt(html: str) -> datetime | None:\n",
    "    \"\"\"Parse publish datetime and return timezone-aware datetime in SGT (UTC+8).\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # 1) JSON-LD (prefer datePublished)\n",
    "    for script in soup.find_all(\"script\", type=\"application/ld+json\"):\n",
    "        try:\n",
    "            data = json.loads(script.string or \"\")\n",
    "        except Exception:\n",
    "            continue\n",
    "        candidates = data if isinstance(data, list) else [data]\n",
    "        for obj in candidates:\n",
    "            if not isinstance(obj, dict):\n",
    "                continue\n",
    "            typ = obj.get(\"@type\") or obj.get(\"@context\")\n",
    "            if (isinstance(typ, str) and \"Article\" in typ) or obj.get(\"headline\") or obj.get(\"datePublished\"):\n",
    "                dt = obj.get(\"datePublished\") or obj.get(\"dateModified\")\n",
    "                if isinstance(dt, str) and ISO_DT_RE.match(dt):\n",
    "                    try:\n",
    "                        dt_parsed = datetime.fromisoformat(dt.replace(\"Z\", \"+00:00\"))\n",
    "                        return dt_parsed.astimezone(SGT)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "    # 2) Meta tags\n",
    "    meta_names = [\n",
    "        (\"meta\", {\"property\": \"article:published_time\"}),\n",
    "        (\"meta\", {\"name\": \"pubdate\"}),\n",
    "        (\"meta\", {\"name\": \"publish-date\"}),\n",
    "        (\"meta\", {\"name\": \"date\"}),\n",
    "        (\"meta\", {\"itemprop\": \"datePublished\"}),\n",
    "        (\"meta\", {\"property\": \"og:pubdate\"}),\n",
    "        (\"meta\", {\"name\": \"parsely-pub-date\"}),\n",
    "        (\"meta\", {\"name\": \"cXenseParse:recs:publishtime\"}),\n",
    "    ]\n",
    "    for tag, attrs in meta_names:\n",
    "        el = soup.find(tag, attrs=attrs)\n",
    "        if el and el.get(\"content\"):\n",
    "            dt = el[\"content\"].strip()\n",
    "            if ISO_DT_RE.match(dt):\n",
    "                try:\n",
    "                    dt_parsed = datetime.fromisoformat(dt.replace(\"Z\", \"+00:00\"))\n",
    "                    return dt_parsed.astimezone(SGT)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    # 3) <time> tags\n",
    "    for t in soup.find_all(\"time\"):\n",
    "        dt = (t.get(\"datetime\") or t.get(\"content\") or \"\").strip()\n",
    "        if ISO_DT_RE.match(dt):\n",
    "            try:\n",
    "                dt_parsed = datetime.fromisoformat(dt.replace(\"Z\", \"+00:00\"))\n",
    "                return dt_parsed.astimezone(SGT)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_article_content(html: str) -> str:\n",
    "    \"\"\"Return cleaned plaintext of the article body, truncated at STOP_MARKER.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # 1) JSON-LD articleBody\n",
    "    try:\n",
    "        for script in soup.find_all(\"script\", type=\"application/ld+json\"):\n",
    "            data = json.loads(script.string or \"\")\n",
    "            blocks = data if isinstance(data, list) else [data]\n",
    "            for obj in blocks:\n",
    "                if isinstance(obj, dict) and obj.get(\"@type\") and \"Article\" in str(obj.get(\"@type\")):\n",
    "                    body = obj.get(\"articleBody\")\n",
    "                    if isinstance(body, str) and len(body.strip()) > 40:\n",
    "                        text = \"\\n\\n\".join(\n",
    "                            _clean_text(x) for x in re.split(r\"\\n{2,}\", body.strip()) if _clean_text(x)\n",
    "                        )\n",
    "                        if text:\n",
    "                            text = _maybe_fix_misencoded(text)\n",
    "                            return _truncate_after_marker(text)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    def collect_paragraphs(container):\n",
    "        paras = []\n",
    "        for p in container.find_all([\"p\", \"h2\", \"h3\"], recursive=True):\n",
    "            if p.find_parent([\"figure\", \"figcaption\", \"aside\", \"blockquote\"]):\n",
    "                continue\n",
    "            raw = \"\".join(t for t in p.stripped_strings if isinstance(t, str))\n",
    "            # Stop if we hit the marker phrase in this paragraph\n",
    "            if STOP_MARKER in raw.lower():\n",
    "                break\n",
    "            txt = _clean_text(raw)\n",
    "            if txt:\n",
    "                paras.append(txt)\n",
    "        return paras\n",
    "\n",
    "    # 2) <article> block\n",
    "    article_tag = soup.find(\"article\")\n",
    "    if article_tag:\n",
    "        for c in article_tag.find_all(attrs={\"itemprop\": \"articleBody\"}):\n",
    "            paras = collect_paragraphs(c)\n",
    "            if len(\" \".join(paras)) > 100:\n",
    "                return _truncate_after_marker(_maybe_fix_misencoded(\"\\n\\n\".join(paras)))\n",
    "        paras = collect_paragraphs(article_tag)\n",
    "        if len(\" \".join(paras)) > 100:\n",
    "            return _truncate_after_marker(_maybe_fix_misencoded(\"\\n\\n\".join(paras)))\n",
    "\n",
    "    # 3) Heuristic containers\n",
    "    for div in soup.find_all([\"div\", \"section\"], class_=True):\n",
    "        classes = \" \".join(div.get(\"class\") or [])\n",
    "        if _CONTENT_CLASS_RE.search(classes):\n",
    "            paras = collect_paragraphs(div)\n",
    "            if len(\" \".join(paras)) > 100:\n",
    "                return _truncate_after_marker(_maybe_fix_misencoded(\"\\n\\n\".join(paras)))\n",
    "\n",
    "    # 4) Last resort: main/body\n",
    "    main_like = soup.find(\"main\") or soup.body\n",
    "    if main_like:\n",
    "        paras = collect_paragraphs(main_like)\n",
    "        if len(\" \".join(paras)) > 100:\n",
    "            return _truncate_after_marker(_maybe_fix_misencoded(\"\\n\\n\".join(paras)))\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def parse_title(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    meta_title = soup.find(\"meta\", property=\"og:title\")\n",
    "    if meta_title and meta_title.get(\"content\"):\n",
    "        return meta_title[\"content\"].strip()\n",
    "    t = soup.find(\"title\")\n",
    "    return t.text.strip() if t else \"\"\n",
    "\n",
    "\n",
    "def is_article_url(url: str) -> bool:\n",
    "    u = urlparse(url)\n",
    "    if u.netloc not in {\"www.channelnewsasia.com\", \"channelnewsasia.com\"}:\n",
    "        return False\n",
    "    path = u.path.lower()\n",
    "    if path.startswith(\"/watch\") or \"/watch/\" in path:\n",
    "        return False\n",
    "    if \"/podcast\" in path or \"/podcasts\" in path or path.startswith(\"/listen\"):\n",
    "        return False\n",
    "    if path.startswith(\"/profile\") or path.startswith(\"/rss\") or path.startswith(\"/weather\"):\n",
    "        return False\n",
    "    if path == \"/\":\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def scrape_article(url: str) -> dict:\n",
    "    html = fetch(url)\n",
    "    if not html:\n",
    "        raise RuntimeError(\"Failed to fetch article HTML.\")\n",
    "    title = parse_title(html)\n",
    "    published_sgt = parse_publish_datetime_sgt(html)\n",
    "    content_text = extract_article_content(html)\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"published_sgt_iso\": published_sgt.isoformat() if published_sgt else \"\",\n",
    "        \"content_text\": content_text,\n",
    "        \"content_chars\": len(content_text) if content_text else 0,\n",
    "    }\n",
    "\n",
    "\n",
    "def read_urls_from_csv(path: str) -> list[str]:\n",
    "    urls = []\n",
    "    with open(path, \"r\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        rows = list(reader)\n",
    "        if not rows:\n",
    "            return urls\n",
    "        header = [h.strip().lower() for h in rows[0]]\n",
    "        data_rows = rows[1:] if any(header) else rows  # headerless CSV\n",
    "        url_idx = 0\n",
    "        if any(header):\n",
    "            if \"url\" in header:\n",
    "                url_idx = header.index(\"url\")\n",
    "            else:\n",
    "                # Fall back to first column if 'url' not found\n",
    "                url_idx = 0\n",
    "        for r in data_rows:\n",
    "            if not r:\n",
    "                continue\n",
    "            u = r[url_idx].strip()\n",
    "            if u:\n",
    "                urls.append(u)\n",
    "    return urls\n",
    "\n",
    "\n",
    "def save_rows_to_excel(rows: list[dict], output_path: str):\n",
    "    ensure_dirs(output_path)\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = \"Articles\"\n",
    "\n",
    "    # Header (added 'content_chars')\n",
    "    headers = [\"title\", \"url\", \"published_sgt_iso\", \"content_text\", \"content_chars\"]\n",
    "    ws.append(headers)\n",
    "\n",
    "    # Rows\n",
    "    for r in rows:\n",
    "        ws.append([r.get(h, \"\") for h in headers])\n",
    "\n",
    "    # Optional: auto width\n",
    "    for column in ws.columns:\n",
    "        max_length = 0\n",
    "        col_letter = column[0].column_letter\n",
    "        for cell in column:\n",
    "            try:\n",
    "                val = \"\" if cell.value is None else str(cell.value)\n",
    "                if len(val) > max_length:\n",
    "                    max_length = len(val)\n",
    "            except Exception:\n",
    "                pass\n",
    "        ws.column_dimensions[col_letter].width = min(max_length + 2, 80)\n",
    "\n",
    "    wb.save(output_path)\n",
    "    print(f\"✅ Saved Excel file to: {output_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(f\"Reading URLs from: {INPUT_CSV}\")\n",
    "    urls = read_urls_from_csv(INPUT_CSV)\n",
    "    print(f\"Found {len(urls)} URL(s).\")\n",
    "\n",
    "    rows = []\n",
    "    failures = []\n",
    "\n",
    "    for i, url in enumerate(urls, 1):\n",
    "        if not is_article_url(url):\n",
    "            print(f\"[{i}/{len(urls)}] Skipping non-article URL: {url}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[{i}/{len(urls)}] Scraping: {url}\")\n",
    "        time.sleep(SLEEP_BETWEEN_REQUESTS)\n",
    "        try:\n",
    "            row = scrape_article(url)\n",
    "            rows.append(row)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {url} | {e}\")\n",
    "            failures.append((url, str(e)))\n",
    "\n",
    "    save_rows_to_excel(rows, OUTPUT_XLSX)\n",
    "\n",
    "    if failures:\n",
    "        print(\"\\nSome URLs failed to scrape:\")\n",
    "        for u, err in failures:\n",
    "            print(f\"- {u} | {err}\")\n",
    "\n",
    "    print(f\"\\nDone. Scraped {len(rows)} article(s).\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134a31e6-6521-4424-bfbf-1a5b2e88b9c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
